{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bdfa9378-e10a-4861-a94e-bc29d1a744c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Flight Delay Modeling: The Puffins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5e2de64a-a054-4e80-a117-5df283b601ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Abstract\n",
    "Flight delays create problems in scheduling for airlines and airports, leading to passenger inconvenience, and huge economic losses. Predicting these delays ahead of time can alleviate some of the issues caused by these delays. Inspired by puffins, birds that are known to be [very punctual](https://www.rspb.org.uk/about-the-rspb/about-us/media-centre/press-releases/rspb-ni-rathlin-punctual-puffins/) in their migratory patterns, our team plans to predict flight departure delays using airport and weather data. Our primary customer for this project are passengers, who we will be able to proactively notify about flight delays 2 hours before the scheduled depature of the flight. We will be predicting whether a flight can be categorized into one of the following four categories. We chose these bins, as this will allow customers to plan their schedules to the airport accordingly.\n",
    " - Will depart on-time (less than 15 minutes after the scheduled departure) \n",
    " - Will depart between 15-44 minutes late\n",
    " - Will depart between 45-89 minutes late\n",
    " - Will depart between greater than 90 minutes late\n",
    "\n",
    "We plan on building models using Logistic Regression and XGBoost to start, though we will be evaluating and assessing other models as we progress through EDA and preliminary pipeline construction. We will report the success of our models using F1-Score, Precision and Recall.\n",
    "\n",
    "\\\n",
    "<img src=\"https://drive.google.com/uc?id=1VNYZJf7ypNNjC24USv9lIZgItLNC3h_z\" alt=\"Google Drive Image\" width=50%/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b7f87410-79b5-4a18-a4ac-6d72dbb32ad2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Team Members\n",
    "\n",
    "Name: Rathin Bector \\\n",
    "Email: rathin.bector@berkeley.edu \\\n",
    "<img src=\"https://drive.google.com/uc?id=13yvUjEUS6aybbRax_wOIffeIIaD4jMNt\" alt=\"Google Drive Image\" width=20%/>\n",
    "\n",
    "Name: Victor Ramirez \\\n",
    "Email: victor.ramirez@berkeley.edu \\\n",
    "<img src=\"https://drive.google.com/uc?id=1NbNsQiYvLQNZdXi0HPEkd3biCRV_eXZY\" alt=\"Google Drive Image\" width=20%/>\n",
    "\n",
    "Name: Francisco Meyo \\\n",
    "Email: francisco@berkeley.edu \\\n",
    "<img src=\"https://drive.google.com/uc?id=1F7qT1sVXMjM_lh1Qr6MnvFF1ZuwTAtaT\" alt=\"Google Drive Image\" width=20%/>\n",
    "\n",
    "Name: Landon Morin \\\n",
    "Email: morinlandon@berkeley.edu \\\n",
    "<img src=\"https://drive.google.com/uc?id=1KLrkBsnwrWNhakBRSRc_Jr2UpEmHLfFN\" alt=\"Google Drive Image\" width=20%/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bfd04f83-5da4-4f6b-b853-ff777ca44eb7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Phase Leader Plan\n",
    "\n",
    "| Phase | Description                                                                                                      | Project Manager |\n",
    "|-------|------------------------------------------------------------------------------------------------------------------|-----------------|\n",
    "| I     | Project Plan, describe datasets, joins, tasks, and metrics                                                       | Rathin Bector   |\n",
    "| II    | EDA, baseline pipeline on all data, Scalability, Efficiency, Distributed/parallel Training, and Scoring Pipeline | Victor Ramirez  |\n",
    "| III   | Feature engineering and hyperparameter tuning, In-class Presentation                                             | Francisco Meyo  |\n",
    "| IV    | Select the optimal algorithm, fine-tune and submit a final report (research style)                               | Landon Morin    |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3e779505-06b2-45dc-98dd-4209b90b4d32",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Credit Assignment Plan\n",
    "| Task Name                                                                                         | Phase   | Assignee                                                    | Due Date   | Status |\n",
    "|---------------------------------------------------------------------------------------------------|---------|-------------------------------------------------------------|------------|--------|\n",
    "| Create Phase 1 Notebook                                                                           | Phase 1 | Rathin Bector                                               | 10/24/2022 | DONE   |\n",
    "| Make Phase Leader Plan                                                                            | Phase 1 | Rathin Bector                                               | 10/26/2022 | DONE   |\n",
    "| Add Pictures and Emails to Notebook                                                               | Phase 1 | Rathin Bector                                               | 10/26/2022 | DONE   |\n",
    "| Project Plan Abstract                                                                             | Phase 1 | Rathin Bector                                               | 10/27/2022 | DONE   |\n",
    "| Credit Assignment Plan                                                                            | Phase 1 | Landon Morin, Rathin Bector, Francisco Meyo, Victor Ramirez | 10/28/2022 | DONE   |\n",
    "| Description of Data                                                                               | Phase 1 | Victor Ramirez, Francisco Meyo, Landon Morin                | 10/29/2022 | DONE   |\n",
    "| Basic EDA writeup                                                                                 | Phase 1 | Landon Morin, Francisco Meyo, Victor Ramirez                | 10/29/2022 | DONE   |\n",
    "| Data Joins Plan                                                                                   | Phase 1 | Rathin Bector, Landon Morin                                 | 10/29/2022 | DONE   |\n",
    "| ML Algorithms and Metrics                                                                         | Phase 1 | Landon Morin, Rathin Bector                                 | 10/30/2022 | DONE   |\n",
    "| Machine Learning Pipelines                                                                        | Phase 1 | Victor Ramirez                                              | 10/30/2022 | DONE   |\n",
    "| Conclusions and Next Steps                                                                        | Phase 1 | Francisco Meyo                                              | 10/30/2022 | DONE   |\n",
    "| Submit Notebook and PDF                                                                           | Phase 1 | Rathin Bector                                               | 10/30/2022 | TO DO  |\n",
    "| Create Post of Discussion Page                                                                    | Phase 1 | Rathin Bector                                               | 10/30/2022 | TO DO  |\n",
    "| EDA on weather table                                                                              | Phase 2 | Francisco Meyo                                              | 11/01/2022 | DOING  |\n",
    "| EDA on airline table                                                                              | Phase 2 | Landon Morin                                                | 11/01/2022 | DOING  |\n",
    "| EDA on stations table                                                                             | Phase 2 | Victor Ramirez                                              | 11/01/2022 | DOING  |\n",
    "| Conduct Data Joins                                                                                | Phase 2 | Rathin Bector                                               | 11/01/2022 | DOING  |\n",
    "| Train, Validation, Test Split                                                                     | Phase 2 | Landon Morin                                                | 11/02/2022 | TO DO  |\n",
    "| Feature Cleanup and Transformations                                                               | Phase 2 | Rathin Bector, Landon Morin, Francisco Meyo, Victor Ramirez | 11/05/2022 | TO DO  |\n",
    "| PCA for Dimensionality Reduction                                                                  | Phase 2 | Rathin Bector                                               | 11/07/2022 | TO DO  |\n",
    "| Logistic Regression Baseline Model                                                                | Phase 2 | Francisco Meyo                                              | 11/08/2022 | TO DO  |\n",
    "| Cross-Validation Scoring Pipeline                                                                 | Phase 2 | Landon Morin                                                | 11/10/2022 | TO DO  |\n",
    "| 2021 Scoring Pipeline                                                                             | Phase 2 | Victor Ramirez                                              | 11/10/2022 | TO DO  |\n",
    "| Run Additional Experiments                                                                        | Phase 2 | Rathin Bector                                               | 11/13/2022 | TO DO  |\n",
    "| Update Phase Project Notebook                                                                     | Phase 2 | Victor Ramirez                                              | 11/13/2022 | TO DO  |\n",
    "| Create 2min Video Update                                                                          | Phase 2 | Victor Ramirez                                              | 11/13/2022 | TO DO  |\n",
    "| Research SMOTE                                                                                    | Phase 4 | Landon Morin                                                | 11/15/2022 | DONE   |\n",
    "| Feature Engineering                                                                               | Phase 3 | Francisco Meyo, Landon Morin, Victor Ramirez                | 11/16/2022 | TO DO  |\n",
    "| Create a baseline model                                                                           | Phase 3 | Rathin Bector                                               | 11/17/2022 | TO DO  |\n",
    "| Conduct test on using new features and report metrics                                             | Phase 3 | Rathin Bector                                               | 11/17/2022 | TO DO  |\n",
    "| Update leaderboard and write a gap analysis of your best pipeline against the Project Leaderboard | Phase 3 | Landon Morin                                                | 11/18/2022 | TO DO  |\n",
    "| Fine-tune baseline pipeline using a grid search                                                   | Phase 3 | Victor Ramirez                                              | 11/18/2022 | TO DO  |\n",
    "| Video update                                                                                      | Phase 3 | Francisco Meyo, Landon Morin, Rathin Bector, Victor Ramirez | 11/20/2022 | TO DO  |\n",
    "| Slides for presentation                                                                           | Phase 3 | Francisco Meyo, Landon Morin, Rathin Bector, Victor Ramirez | 11/20/2022 | TO DO  |\n",
    "| Consider other models and build pipelines for these models                                        | Phase 4 | Victor Ramirez, Francisco Meyo, Rathin Bector, Landon Morin | 11/22/2022 | TO DO  |\n",
    "| Hyperparameter tuning for all models using cross-validation                                       | Phase 4 | Rathin Bector                                               | 11/26/2022 | TO DO  |\n",
    "| Final feature engineering and refinement                                                          | Phase 4 | Landon Morin                                                | 11/26/2022 | TO DO  |\n",
    "| Consider and formalize written analysis of exciting, novel directions that we pursued             | Phase 4 | Victor Ramirez                                              | 11/26/2022 | TO DO  |\n",
    "| Clean up code                                                                                     | Phase 4 | Francisco Meyo                                              | 12/04/2022 | TO DO  |\n",
    "| Gap analysis of best pipeline against project leaderboard                                         | Phase 4 | Landon Morin                                                | 12/04/2022 | TO DO  |\n",
    "| Final Writeup                                                                                     | Phase 4 | Landon Morin, Victor Ramirez, Francisco Meyo, Rathin Bector | 12/04/2022 | TO DO  |\n",
    "| Submission and discussion board post                                                              | Phase 4 | Landon Morin                                                | 12/04/2022 | TO DO  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "60957c08-e347-4a8f-9fb3-b0c0e63700a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Project Plan\n",
    "\n",
    "We are using ClickUp for project planning for this project. The Clickup folder for this project can be found [here](https://sharing.clickup.com/42080451/g/h/184663-160/d7cc34e69aa3512). Below, you can find the Gantt charts we have for each phase of this project.\n",
    "\n",
    "### Phase 1\n",
    "<img src=\"https://drive.google.com/uc?id=1igtHkkr-dC7tVNMZ3uk_Q2kvqATs4KDB\" alt=\"Google Drive Image\" width=90%/>\n",
    "\n",
    "### Phase 2\n",
    "<img src=\"https://drive.google.com/uc?id=1iMnTRgQCdYP30K9udHHF7KhbZ6DyDMrg\" alt=\"Google Drive Image\" width=90%/>\n",
    "\n",
    "### Phase 3\n",
    "<img src=\"https://drive.google.com/uc?id=1lcFOfhD9nK0X_jTPk6HvpnJH-PHYkNhP\" alt=\"Google Drive Image\" width=90%/>\n",
    "\n",
    "### Phase 4\n",
    "<img src=\"https://drive.google.com/uc?id=1_0ZIc9Jt-9N01KCk27O3q1cq0srAAwtL\" alt=\"Google Drive Image\" width=90%/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aa6cfb51-f079-4704-8ded-57cb793128b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Description of Data \n",
    "### Airlines Dataset\n",
    "The provided airlines datatable is maintained by the US Department of Transportation and consists of 109 columns and 74,177,433 rows of flight data from 2015 to 2021, which are comprised of integer, strings, and double-type data. This dataset contains both continuous and categorical data, and a significant portion of the dataset contains null values. Out of 109 columns, we determined that 53 contained too many null values for further use in our experiments. All of the remaining 56 columns have less than 2% null values, however correlations between some of the remaining feature columns (i.e. origin airport and origin airport city) will further reduce the usable features. \n",
    "\n",
    "For the machine learning pipelines, we have decided to employ DEP_DELAY_GROUP as labels since this column pre-buckets delay times for multiclass classification. We will use a join of MONTH and DAY_OF_MONTH, OP_CARRIER (categorical; airline identifier), ORIGIN_AIRPORT_ID (categorical), and DISTANCE (numerical) as features. Since ARR_DELAY (categorical) is current, it cannot be used to predict departure delays without some feature engineering. Therefore, we will use TAIL_NUMBER (categorical), ORIGIN_AIRPORT_ID (categorical), DEST_AIRPORT_ID (categorical), and time to engineer ARR_DELAY_GROUP as a useable predictive feature variable for future flight delays. \n",
    "\n",
    "### Weather dataset\n",
    "\n",
    "The weather dataset comprises 124 fields detailing data such as Daily Average Temperature, Sea Level Pressure, Wind Speed, and Relative Humidity, among others, observed for a variety of places during our sampling period in the first quarter of 2015. \n",
    "\n",
    "Upon further analysis, we observed that 123 of the 124 fields of the dataset were of string data type and that 109 fields had 80% or more missing values. Based on the above, we performed the following:  \n",
    "\n",
    "1. Disregarded the 109 fields with 80% or more Null values given that we believe that any attempt to impute any missing data could result in artificially transforming its nature.\n",
    "2. Cast the 15 remaining features to the appropriate data type. \n",
    "\n",
    "Upon examination of the resulting dataset, and contingent on a deeper data exploration and analysis to be performed in Phase II, we concluded that the following features could be used in our model (either as predictors or identifiers for joining with other datasets):\n",
    "\n",
    "STATION: weather station identifier  \n",
    "DATE: timestamp of the record  \n",
    "LATITUDE: latitude coordinates   \n",
    "LONGITUDE: longitude coordinates   \n",
    "ELEVATION:  elevation relative to mean sea level  \n",
    "NAME: name of the meteorological-point-observation  \n",
    "HourlyDewPointTemperature: the temperature the air needs to be cooled to (at constant pressure) in order to achieve a relative humidity (RH) of 100%  \n",
    "HourlyDryBulbTemperature: ambient air temperature. It is called \"Dry Bulb\" because the air temperature is indicated by a thermometer not affected by the moisture of the air.  \n",
    "HourlyRelativeHumidity:  it is the amount of water vapor present in air expressed as a percentage (%RH) of the amount needed to achieve saturation at the same temperature.  \n",
    "HourlyWindDirection: wind direction in tens of degrees  \n",
    "HourlyWindSpeed: wind speed expressed in Beaufort scale   \n",
    "\n",
    "### Weather Stations Dataset\n",
    "\n",
    "The provided weather station dataset contains observations taken by U.S. Air Force personnel at bases in the United States. Stations are assigned a WBAN (Weather Bureau Army Navy) number. Original forms sent from the Air Force to NCDC by agreement and stored in the NCDC archives. The data consists of 12 colums and 5,004,169 rows of weather station data. The data features consist of strings and doubles with no NAN or NULL values. \n",
    "\n",
    "For the data develpment, we will are planning on joining this dataset using an external mapping table. We will be utilizing the stations latitude and longitude coordinates to find the nearest airport to the weather stations. \n",
    "\n",
    "We will take advantage of the neighbor_lat, neighbor_lon and distance_toneighbor to explore the idea of creating a distance graph. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "33ff9728-6725-4b31-83c4-6c69ff5dfdd2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Exploratory Data Analysis\n",
    "\n",
    "## Weather Staton Plot\n",
    "\n",
    "In reviewing the station data, we discovered the latitude, longitude columns for all the weather stations. Using the latitude and longitude columns we generated a table with all the required coordinates. Leveraging the data bricks Map (Marker) visualization tool, we plotted all the station coordinates on a satellite layer. \n",
    "\n",
    "\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?id=12KwK_cdQW0WhIXDRCHol8bxkpMwqxYWj\" alt=\"Google Drive Image\" width=90%/>\n",
    "\n",
    "## Airlines EDA\n",
    "The airlines dataset is characterized by high null values in about half of the columns. For the columns that remain after dropping columns with high nulls, we are left with columns with 2% null values or fewer. In the following table, we observe the percent of null values in the remaining columns. Note that the remainder of our kept features contain no null values.\n",
    "\n",
    "Remaining Columns With Null Values | Remaining Columns With No Nulls\n",
    "-|-\n",
    "<img src=\"https://drive.google.com/uc?id=1KoNxWoZB5Ptl6WtwLog-3F8jB3sQFkXg\" alt=\"Google Drive Image\" width=90%/> | <img src=\"https://drive.google.com/uc?id=1nD0OoIV4bJU9ijWHao7xo386EY1oHqwZ\" alt=\"Google Drive Image\" width=90%/>\n",
    "\n",
    "Of these columns, we map the descriptive statistics of each numerical feature below. From this brief analysis we can begin to see skew in features such as flight delay, year (geopolitical and health factors in 2020), flight arrival, flight distance, flight airtime, and diverted flights. Further analysis in phase 2 will provide us with the necessary information to take actions to address imbalances in both numerical and categorical data. Furthermore, differing numerical scales will require normalization of the training and test sets such that gradient descent and loss optimization is possible.\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?id=1YWXDgy3Rntkgn-IxXexL9r7KnROASFOx\" alt=\"Google Drive Image\" width=30%/>\n",
    "\n",
    "\n",
    "As previously mentioned, our labels will be taken from DEP_DELAY_GROUP, which has a heavy right skew. This indicates that we will need to use SMOTE to adjust for class imbalances.\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?id=17xi9M-oImTFDFMdSuIud9wzlhNFzqMUs\" alt=\"Google Drive Image\" width=90%/>\n",
    "\n",
    "From preliminary analyses, we find that there are potential patterns between our selected features and labels that will provide predictive power despite class imbalances.. \n",
    "\n",
    "Relationship of Sampled Airlines With Grouped Departure Delays | Relationship Of Total Distance With Total Flight Delay\n",
    "-|-\n",
    "<img src=\"https://drive.google.com/uc?id=1cGyGUFGyo7CbeDNQJYUmXVd6pPNYMfGd\" alt=\"Google Drive Image\" width=90%/> | <img src=\"https://drive.google.com/uc?id=1EHG5MpWRbQOcSA5ihu4sugSaaS0hGXVj\" alt=\"Google Drive Image\" width=90%/>\n",
    "\n",
    "## Weather EDA\n",
    "\n",
    "Selected features of the weather dataset contain several string fields that will require further analysis for categorization. On the other hand, numeric fields are included in several scales that will make normalization necessary:  \n",
    "\n",
    "\n",
    "Summary Statistics P.1 | Summary Statistic P.2\n",
    "-|-\n",
    "<img src=\"https://drive.google.com/uc?id=1BbKOPtB9d_zLrPx2k7mx3u2S0o_AUHsl\" alt=\"Google Drive Image\" width=90%/> | <img src=\"https://drive.google.com/uc?id=1yhMlL92fRLnYr_Ue9TVD4K5cZke_a5Hv\" alt=\"Google Drive Image\" width=90%/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "43ab416c-617b-40ce-8756-5efd27b85a8c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Data Joins\n",
    "\n",
    "In order to have features derived from the weather dataset to predict flight, we must join the weather data to the airlines data. This join can be performed in the following steps:\n",
    "1. The [Master Coordinate table](https://www.transtats.bts.gov/Fields.asp?gnoyr_VQ=FLL), available on the Beaureau of Transportation Statistics website, contains the latitude an longitude for each airport in the world. We can find the closest weather station in the stations table to each airport using Haversine Distance formula Analysis. A guide on how to do this can be found [here](https://medium.com/analytics-vidhya/finding-nearest-pair-of-latitude-and-longitude-match-using-python-ce50d62af546) \n",
    "2. Left join the airlines table with the table derived from step 1, on `ORIGIN_AIRPORT_SEQ_ID` and `AIRPORT_SEQ_ID` respectively. This will give us the closest weather station to the origin ariport for each flight. \n",
    "3. Left join the the table from step 2 with the weather table, on the `station_id` and `STATION` as well as the `FL_DATE` and `DATE` columns respectively. This gives us the weather at the origin airport the day of each flight."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9ced805a-4663-494e-8ab2-869702709b1e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Machine Learning Algorithms and Metrics\n",
    "We will be operationalizing the problem of predicting flight delays for consumers using multiclass classification. To accomplish this, we will create a hybrid label from DEP_DELAY_GROUP, which consists of bins of 15 minute delay intervals. Since DEP_DELAY_GROUP is skewed toward lower departure delays, we will consider anything less than 15 minutes delayed as on time, then will bin delays into 30 minute classification intervals until 90 minutes, with all times after 90 minutes merged into one bucket +90. \n",
    "\n",
    "Our metric for success will be the F1 score, since consumers will both care that we are making accurate delay predictions (precision), but also that we are minimizing false negatives(recall). The F1 score is a harmonic mean of both precision and recall, and therefore considers both metrics in its calculation. \n",
    "\n",
    "$$Equation one: F1 = 2 * \\frac{Precision * Recall}{Precision + Recall}$$\n",
    "\n",
    "To start, we will use a baseline algorithm that predicts that a flight will not be delayed. Since most flights are not delayed, we predict that this will be reasonably accurate, but will have a zero recall, precision, and F1 score. Against this baseline, we will be using three machine learning algorithms. \n",
    "\n",
    "Model one will be logistic regression with a momentum optimizer to predict the probability of a flight being classified as k-minutes delayed, where k is a bucket that is comprised of 15 minute intervals up to 90 minutes and infinite minutes after 90 minutes. We can represent this problem as the following:\n",
    "\n",
    "$$Model One - Logistic Regression: \\hat{y} = argmax f_k(x)$$ Where k represents the following:\n",
    "\n",
    "k | Bucket\n",
    "- | -\n",
    "1 | delay <= 15 min\n",
    "2 | 15 min > delay >= 45 min\n",
    "3 | 45 min > delay >= 75 min\n",
    "4 | 75 min > delay >= 90 min\n",
    "5 | 90 min > delay\n",
    "\n",
    "\n",
    "\n",
    "We will use a multiclass Binary Cross Entropy loss function, which can be represented by the following function:\n",
    "\n",
    "$$Model One - BCE Loss: -\\sum_{c=1}^My_{o,c}\\log(p_{o,c})$$ Where M represents the number of classes, y represents the binary classification of class c, and p represents the probability of the classification of class c. \n",
    "\n",
    "Model 2 will be XGBoost, also with a Binary Cross Entropy loss function. XGBoost is known to scale well, and to provide more accurate results than a simple random forest. XGBoost improves upon each iteration by considering and weighting underperforming predictions from the previous iteration. This algorithm is better performing than random forests when operating on imbalanced data. This feature will be necessary for a large and imbalanced dataset that consists of mostly on-time flights. The XGBoost loss function can be represented by the following function:\n",
    "$$Model Two - BCE Loss: -\\sum_{c=1}^My_{o,c}\\log(p_{o,c})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1de1be42-949b-451f-8c59-818ecd4b44de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Machine Learning Pipelines\n",
    "\n",
    "We will following the industry standard machine learning pipeline that is an end-to-end process. A machine learning pipeline is a well-defined set of steps to develop, train, test, and optimize a machine learning algorithm. \n",
    "\n",
    "Each stage of the machine learning pipeline makes up a specific step in the entire pipeline. The workflow is broken up into modular stages of work. The stages are independent and can be optimized. \n",
    "\n",
    "The pipeline begins with the ingestion and flow of raw data into the pipeline. Once the data is cleaned, sanitized the dataflow continues to the next stage. \n",
    "\n",
    "The following stage will handle the feature engineering portion of the pipeline. This stage will handle the process of selecting, manipulating, and transforming the data into features that can be used in machine learning model. \n",
    "\n",
    "The next stage is the model development, training, testing, and tuning. This stage will use the data split between train and test sets. Once the model is trained and tested the model validation happens based on standard testing metrics and results. \n",
    "\n",
    "In the final stage the machine learning model is used on new data.\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?id=1tho8U5UBa20AMBZidTTtJ6hiMkmoRENC\" alt=\"Google Drive Image\" width=90%/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1a8c0f4b-d3a0-453f-905a-730fab626e70",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Conclusion and Next Steps\n",
    "\n",
    "The work conducted during Phase I allowed us to: \n",
    "\n",
    "1. Obtain clean datasets (eg performing simple transformations or disregarding features with a high content of missing values)  \n",
    "2. Selec relevant features of each dataset after performing basic EDA  \n",
    "3. Join datasets to enhance the features available for our model  \n",
    "4. Define preliminary models and performance metrics  \n",
    "\n",
    "\n",
    "During Phase II we will be conducting the following next steps:  \n",
    "1. Perform a detailed EDA that will allow us to define any normalization needed as well as parameter fine tunning   \n",
    "2. Determine and calculate our baseline model (most likely Logistic regression)  \n",
    "3. Perform experiments   \n",
    "4. Adjust our plan as needed"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Section2_Group3_Phase1",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}